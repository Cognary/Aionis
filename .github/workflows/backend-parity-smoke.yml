name: Backend Parity Smoke

on:
  workflow_dispatch:
  pull_request:
    branches:
      - main
  push:
    branches:
      - main

jobs:
  parity-smoke:
    runs-on: ubuntu-latest
    timeout-minutes: 35
    strategy:
      fail-fast: false
      matrix:
        backend:
          - postgres
          - embedded

    env:
      DATABASE_URL: postgres://aionis:aionis@127.0.0.1:5432/aionis_memory
      PORT: "3101"
      MEMORY_STORE_BACKEND: ${{ matrix.backend }}
      MEMORY_STORE_EMBEDDED_EXPERIMENTAL_ENABLED: ${{ matrix.backend == 'embedded' && 'true' || 'false' }}
      MEMORY_STORE_EMBEDDED_SNAPSHOT_MAX_BYTES: ${{ matrix.backend == 'embedded' && '12000' || '52428800' }}
      MEMORY_STORE_EMBEDDED_SNAPSHOT_COMPACTION_ENABLED: "true"
      MEMORY_STORE_EMBEDDED_SNAPSHOT_COMPACTION_MAX_ROUNDS: "8"
      MEMORY_STORE_EMBEDDED_SHADOW_MIRROR_ENABLED: "false"
      MEMORY_STORE_EMBEDDED_RECALL_DEBUG_EMBEDDINGS_ENABLED: "false"
      MEMORY_STORE_EMBEDDED_RECALL_AUDIT_ENABLED: "true"
      MEMORY_STORE_EMBEDDED_SESSION_GRAPH_ENABLED: "true"
      MEMORY_STORE_EMBEDDED_PACK_EXPORT_ENABLED: "true"
      MEMORY_STORE_EMBEDDED_PACK_IMPORT_ENABLED: "true"
      EMBEDDED_SNAPSHOT_MAX_DROPPED_NODES_GUARD: "32"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: 20
          cache: npm

      - name: Install deps
        run: npm ci

      - name: Prepare env file
        run: |
          cp .env.example .env
          {
            echo "DATABASE_URL=${DATABASE_URL}"
            echo "PORT=${PORT}"
            echo "MEMORY_STORE_BACKEND=${MEMORY_STORE_BACKEND}"
            echo "MEMORY_STORE_EMBEDDED_EXPERIMENTAL_ENABLED=${MEMORY_STORE_EMBEDDED_EXPERIMENTAL_ENABLED}"
            echo "MEMORY_STORE_EMBEDDED_SNAPSHOT_MAX_BYTES=${MEMORY_STORE_EMBEDDED_SNAPSHOT_MAX_BYTES}"
            echo "MEMORY_STORE_EMBEDDED_SNAPSHOT_COMPACTION_ENABLED=${MEMORY_STORE_EMBEDDED_SNAPSHOT_COMPACTION_ENABLED}"
            echo "MEMORY_STORE_EMBEDDED_SNAPSHOT_COMPACTION_MAX_ROUNDS=${MEMORY_STORE_EMBEDDED_SNAPSHOT_COMPACTION_MAX_ROUNDS}"
            echo "MEMORY_STORE_EMBEDDED_SHADOW_MIRROR_ENABLED=${MEMORY_STORE_EMBEDDED_SHADOW_MIRROR_ENABLED}"
            echo "MEMORY_STORE_EMBEDDED_RECALL_DEBUG_EMBEDDINGS_ENABLED=${MEMORY_STORE_EMBEDDED_RECALL_DEBUG_EMBEDDINGS_ENABLED}"
            echo "MEMORY_STORE_EMBEDDED_RECALL_AUDIT_ENABLED=${MEMORY_STORE_EMBEDDED_RECALL_AUDIT_ENABLED}"
            echo "MEMORY_STORE_EMBEDDED_SESSION_GRAPH_ENABLED=${MEMORY_STORE_EMBEDDED_SESSION_GRAPH_ENABLED}"
            echo "MEMORY_STORE_EMBEDDED_PACK_EXPORT_ENABLED=${MEMORY_STORE_EMBEDDED_PACK_EXPORT_ENABLED}"
            echo "MEMORY_STORE_EMBEDDED_PACK_IMPORT_ENABLED=${MEMORY_STORE_EMBEDDED_PACK_IMPORT_ENABLED}"
          } >> .env

      - name: Start db
        run: docker compose up -d db

      - name: Wait for db
        run: |
          for i in {1..60}; do
            if docker compose exec -T db pg_isready -U aionis -d aionis_memory >/dev/null 2>&1; then
              exit 0
            fi
            sleep 2
          done
          echo "db not ready in time" >&2
          exit 1

      - name: Run migrations
        run: docker compose run --rm migrate

      - name: Contract smoke
        run: npm run -s test:contract

      - name: Startup health + API parity smoke
        run: |
          if [[ "${MEMORY_STORE_BACKEND}" == "embedded" ]]; then
            export MEMORY_STORE_EMBEDDED_PACK_EXPORT_ENABLED=false
          fi

          node dist/index.js >/tmp/backend_parity_api.log 2>&1 &
          API_PID=$!
          echo "${API_PID}" >/tmp/backend_parity_api.pid

          for i in {1..60}; do
            if curl -fsS "http://127.0.0.1:${PORT}/health" >/tmp/backend_parity_health.json 2>/dev/null; then
              break
            fi
            sleep 1
          done

          if [[ ! -s /tmp/backend_parity_health.json ]]; then
            echo "health endpoint never became ready" >&2
            sed -n '1,220p' /tmp/backend_parity_api.log >&2 || true
            exit 1
          fi

          node -e '
            const fs = require("fs");
            const p = JSON.parse(fs.readFileSync("/tmp/backend_parity_health.json", "utf8"));
            const expected = process.env.MEMORY_STORE_BACKEND;
            if (p.ok !== true) throw new Error("health ok must be true");
            if (p.memory_store_backend !== expected) throw new Error(`backend mismatch: ${p.memory_store_backend} != ${expected}`);
            if (typeof p.recall_store_access_capability_version !== "number") throw new Error("missing recall capability version");
            if (typeof p.write_store_access_capability_version !== "number") throw new Error("missing write capability version");
            const recallCaps = p.memory_store_recall_capabilities;
            if (!recallCaps || typeof recallCaps !== "object") throw new Error("missing recall capability object");
            if (typeof recallCaps.debug_embeddings !== "boolean") throw new Error("missing recall capability debug_embeddings");
            if (typeof recallCaps.audit_insert !== "boolean") throw new Error("missing recall capability audit_insert");
            const expectedDebugEmbeddings = expected === "postgres" || String(process.env.MEMORY_STORE_EMBEDDED_RECALL_DEBUG_EMBEDDINGS_ENABLED || "false") === "true";
            const expectedAuditInsert = expected === "postgres" || String(process.env.MEMORY_STORE_EMBEDDED_RECALL_AUDIT_ENABLED || "false") === "true";
            if (recallCaps.debug_embeddings !== expectedDebugEmbeddings) {
              throw new Error(`recall capability mismatch debug_embeddings: ${recallCaps.debug_embeddings} != ${expectedDebugEmbeddings}`);
            }
            if (recallCaps.audit_insert !== expectedAuditInsert) {
              throw new Error(`recall capability mismatch audit_insert: ${recallCaps.audit_insert} != ${expectedAuditInsert}`);
            }
            const writeCaps = p.memory_store_write_capabilities;
            if (!writeCaps || typeof writeCaps !== "object") throw new Error("missing write capability object");
            if (typeof writeCaps.shadow_mirror_v2 !== "boolean") throw new Error("missing write capability shadow_mirror_v2");
            const expectedShadowMirror = expected === "postgres" || String(process.env.MEMORY_STORE_EMBEDDED_SHADOW_MIRROR_ENABLED || "false") === "true";
            if (writeCaps.shadow_mirror_v2 !== expectedShadowMirror) {
              throw new Error(`write capability mismatch shadow_mirror_v2: ${writeCaps.shadow_mirror_v2} != ${expectedShadowMirror}`);
            }
            const featureCaps = p.memory_store_feature_capabilities;
            if (!featureCaps || typeof featureCaps !== "object") throw new Error("missing feature capability object");
            if (typeof featureCaps.sessions_graph !== "boolean") throw new Error("missing feature capability sessions_graph");
            if (typeof featureCaps.packs_export !== "boolean") throw new Error("missing feature capability packs_export");
            if (typeof featureCaps.packs_import !== "boolean") throw new Error("missing feature capability packs_import");
            const expectedSessionGraph = expected === "postgres" || String(process.env.MEMORY_STORE_EMBEDDED_SESSION_GRAPH_ENABLED || "false") === "true";
            const expectedPackExport = expected === "postgres" || String(process.env.MEMORY_STORE_EMBEDDED_PACK_EXPORT_ENABLED || "false") === "true";
            const expectedPackImport = expected === "postgres" || String(process.env.MEMORY_STORE_EMBEDDED_PACK_IMPORT_ENABLED || "false") === "true";
            if (featureCaps.sessions_graph !== expectedSessionGraph) {
              throw new Error(`feature capability mismatch sessions_graph: ${featureCaps.sessions_graph} != ${expectedSessionGraph}`);
            }
            if (featureCaps.packs_export !== expectedPackExport) {
              throw new Error(`feature capability mismatch packs_export: ${featureCaps.packs_export} != ${expectedPackExport}`);
            }
            if (featureCaps.packs_import !== expectedPackImport) {
              throw new Error(`feature capability mismatch packs_import: ${featureCaps.packs_import} != ${expectedPackImport}`);
            }
            const contract = p.memory_store_capability_contract;
            if (!contract || typeof contract !== "object") throw new Error("missing capability contract object");
            if (!contract.shadow_mirror_v2 || contract.shadow_mirror_v2.failure_mode !== "soft_degrade") {
              throw new Error("capability contract mismatch for shadow_mirror_v2");
            }
            if (!contract.sessions_graph || contract.sessions_graph.failure_mode !== "hard_fail") {
              throw new Error("capability contract mismatch for sessions_graph");
            }
            if (expected === "embedded") {
              if (typeof p.memory_store_embedded_snapshot_max_bytes !== "number") throw new Error("embedded health missing snapshot max bytes");
              if (!p.memory_store_embedded_snapshot_metrics || typeof p.memory_store_embedded_snapshot_metrics !== "object") {
                throw new Error("embedded health missing snapshot metrics object");
              }
            }
          '

          # Deterministic parity case: write one shared node with ready embedding, then recall with same query embedding.
          node -e '
            const fs = require("fs");
            const vec = Array.from({ length: 1536 }, () => 0);
            const writePayload = {
              tenant_id: "parity",
              scope: "backend_parity_smoke",
              actor: "ci",
              input_text: "backend parity smoke write",
              auto_embed: false,
              memory_lane: "shared",
              nodes: [
                {
                  client_id: "parity-node-1",
                  type: "event",
                  title: "Parity Event",
                  text_summary: "Backend parity smoke event",
                  embedding: vec
                }
              ],
              edges: []
            };
            const recallPayload = {
              tenant_id: "parity",
              scope: "backend_parity_smoke",
              query_embedding: vec,
              limit: 5,
              neighborhood_hops: 1
            };
            fs.writeFileSync("/tmp/backend_parity_write.json", JSON.stringify(writePayload));
            fs.writeFileSync("/tmp/backend_parity_recall.json", JSON.stringify(recallPayload));
          '

          curl -fsS -X POST "http://127.0.0.1:${PORT}/v1/memory/write" \
            -H "content-type: application/json" \
            --data-binary "@/tmp/backend_parity_write.json" >/tmp/backend_parity_write_out.json

          curl -fsS -X POST "http://127.0.0.1:${PORT}/v1/memory/recall" \
            -H "content-type: application/json" \
            --data-binary "@/tmp/backend_parity_recall.json" >/tmp/backend_parity_recall_out.json

          node -e '
            const fs = require("fs");
            const writeOut = JSON.parse(fs.readFileSync("/tmp/backend_parity_write_out.json", "utf8"));
            const recallOut = JSON.parse(fs.readFileSync("/tmp/backend_parity_recall_out.json", "utf8"));
            if (typeof writeOut.commit_id !== "string" || writeOut.commit_id.length === 0) {
              throw new Error("write response missing commit_id");
            }
            if (!Array.isArray(writeOut.nodes) || writeOut.nodes.length < 1) {
              throw new Error("write response nodes should be non-empty");
            }
            if (!Array.isArray(recallOut.seeds)) throw new Error("recall seeds missing");
            if (!recallOut.subgraph || !Array.isArray(recallOut.subgraph.nodes)) {
              throw new Error("recall subgraph nodes missing");
            }
            if (recallOut.seeds.length < 1) {
              throw new Error("recall returned no seeds for parity case");
            }
          '

          npm run -s sdk:smoke >/tmp/backend_parity_sdk_smoke.json
          npm run -s sdk:py:smoke >/tmp/backend_parity_py_sdk_smoke.json

          curl -fsS "http://127.0.0.1:${PORT}/health" >/tmp/backend_parity_health_after.json

          node -e '
            const fs = require("fs");
            const expected = process.env.MEMORY_STORE_BACKEND;
            if (expected !== "embedded") process.exit(0);
            const before = JSON.parse(fs.readFileSync("/tmp/backend_parity_health.json", "utf8"));
            const after = JSON.parse(fs.readFileSync("/tmp/backend_parity_health_after.json", "utf8"));
            const b = before.memory_store_embedded_snapshot_metrics || {};
            const a = after.memory_store_embedded_snapshot_metrics || {};
            const maxRounds = Number(process.env.MEMORY_STORE_EMBEDDED_SNAPSHOT_COMPACTION_MAX_ROUNDS || "8");
            const maxBytes = Number(process.env.MEMORY_STORE_EMBEDDED_SNAPSHOT_MAX_BYTES || "0");
            const maxDroppedNodes = Number(process.env.EMBEDDED_SNAPSHOT_MAX_DROPPED_NODES_GUARD || "32");

            if (typeof a.persist_total !== "number") throw new Error("embedded metrics missing persist_total");
            if (a.persist_total < Number(b.persist_total || 0) + 1) {
              throw new Error(`embedded persist_total did not advance after write: before=${b.persist_total} after=${a.persist_total}`);
            }
            if (typeof a.last_bytes_before_compaction !== "number" || typeof a.last_bytes_after_compaction !== "number") {
              throw new Error("embedded metrics missing compaction byte counters");
            }
            if (a.last_bytes_after_compaction > a.last_bytes_before_compaction) {
              throw new Error("embedded compaction should not increase snapshot size");
            }
            if (a.last_compaction && typeof a.last_compaction.rounds === "number" && a.last_compaction.rounds > maxRounds) {
              throw new Error(`embedded compaction rounds exceeded guard: ${a.last_compaction.rounds} > ${maxRounds}`);
            }
            if (a.last_bytes_before_compaction > maxBytes && (!a.last_compaction || a.last_compaction.applied !== true)) {
              throw new Error("embedded compaction should be marked applied when snapshot exceeds max bytes");
            }
            if (a.last_compaction && Number(a.last_compaction.dropped_nodes || 0) > maxDroppedNodes) {
              throw new Error(
                `embedded compaction dropped too many nodes in one persist: ${a.last_compaction.dropped_nodes} > ${maxDroppedNodes}`,
              );
            }
          '

          kill "${API_PID}" || true
          wait "${API_PID}" || true

      - name: Diagnostics on failure
        if: failure()
        run: |
          docker compose ps || true
          docker compose logs --no-color db || true
          sed -n '1,220p' /tmp/backend_parity_api.log || true

      - name: Cleanup
        if: always()
        run: docker compose down -v
