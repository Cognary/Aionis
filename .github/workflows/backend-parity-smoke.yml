name: Backend Parity Smoke

on:
  workflow_dispatch:
  pull_request:
    branches:
      - main
  push:
    branches:
      - main

jobs:
  parity-smoke:
    name: parity-smoke (${{ matrix.profile }})
    runs-on: ubuntu-latest
    timeout-minutes: 35
    strategy:
      fail-fast: false
      matrix:
        include:
          - profile: postgres
            backend: postgres
            embedded_experimental_enabled: "false"
            embedded_snapshot_max_bytes: "52428800"
            embedded_session_graph_enabled: "true"
            embedded_pack_export_enabled: "true"
            embedded_pack_import_enabled: "true"
          - profile: embedded_capability_off
            backend: embedded
            embedded_experimental_enabled: "true"
            embedded_snapshot_max_bytes: "12000"
            embedded_session_graph_enabled: "false"
            embedded_pack_export_enabled: "false"
            embedded_pack_import_enabled: "false"
          - profile: embedded_feature_enabled
            backend: embedded
            embedded_experimental_enabled: "true"
            embedded_snapshot_max_bytes: "12000"
            embedded_session_graph_enabled: "true"
            embedded_pack_export_enabled: "true"
            embedded_pack_import_enabled: "true"

    env:
      DATABASE_URL: postgres://aionis:aionis@127.0.0.1:5432/aionis_memory
      PORT: "3101"
      ADMIN_TOKEN: ci-admin-token
      MEMORY_STORE_BACKEND: ${{ matrix.backend }}
      MEMORY_STORE_EMBEDDED_EXPERIMENTAL_ENABLED: ${{ matrix.embedded_experimental_enabled }}
      MEMORY_STORE_EMBEDDED_SNAPSHOT_MAX_BYTES: ${{ matrix.embedded_snapshot_max_bytes }}
      MEMORY_STORE_EMBEDDED_SNAPSHOT_COMPACTION_ENABLED: "true"
      MEMORY_STORE_EMBEDDED_SNAPSHOT_COMPACTION_MAX_ROUNDS: "8"
      MEMORY_STORE_EMBEDDED_SHADOW_MIRROR_ENABLED: "false"
      MEMORY_STORE_EMBEDDED_RECALL_DEBUG_EMBEDDINGS_ENABLED: "false"
      MEMORY_STORE_EMBEDDED_RECALL_AUDIT_ENABLED: "true"
      MEMORY_STORE_EMBEDDED_SESSION_GRAPH_ENABLED: ${{ matrix.embedded_session_graph_enabled }}
      MEMORY_STORE_EMBEDDED_PACK_EXPORT_ENABLED: ${{ matrix.embedded_pack_export_enabled }}
      MEMORY_STORE_EMBEDDED_PACK_IMPORT_ENABLED: ${{ matrix.embedded_pack_import_enabled }}
      EMBEDDED_SNAPSHOT_MAX_DROPPED_NODES_GUARD: "32"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: 20
          cache: npm

      - name: Install deps
        run: npm ci

      - name: Prepare env file
        run: |
          cp .env.example .env
          {
            echo "DATABASE_URL=${DATABASE_URL}"
            echo "PORT=${PORT}"
            echo "ADMIN_TOKEN=${ADMIN_TOKEN}"
            echo "MEMORY_STORE_BACKEND=${MEMORY_STORE_BACKEND}"
            echo "MEMORY_STORE_EMBEDDED_EXPERIMENTAL_ENABLED=${MEMORY_STORE_EMBEDDED_EXPERIMENTAL_ENABLED}"
            echo "MEMORY_STORE_EMBEDDED_SNAPSHOT_MAX_BYTES=${MEMORY_STORE_EMBEDDED_SNAPSHOT_MAX_BYTES}"
            echo "MEMORY_STORE_EMBEDDED_SNAPSHOT_COMPACTION_ENABLED=${MEMORY_STORE_EMBEDDED_SNAPSHOT_COMPACTION_ENABLED}"
            echo "MEMORY_STORE_EMBEDDED_SNAPSHOT_COMPACTION_MAX_ROUNDS=${MEMORY_STORE_EMBEDDED_SNAPSHOT_COMPACTION_MAX_ROUNDS}"
            echo "MEMORY_STORE_EMBEDDED_SHADOW_MIRROR_ENABLED=${MEMORY_STORE_EMBEDDED_SHADOW_MIRROR_ENABLED}"
            echo "MEMORY_STORE_EMBEDDED_RECALL_DEBUG_EMBEDDINGS_ENABLED=${MEMORY_STORE_EMBEDDED_RECALL_DEBUG_EMBEDDINGS_ENABLED}"
            echo "MEMORY_STORE_EMBEDDED_RECALL_AUDIT_ENABLED=${MEMORY_STORE_EMBEDDED_RECALL_AUDIT_ENABLED}"
            echo "MEMORY_STORE_EMBEDDED_SESSION_GRAPH_ENABLED=${MEMORY_STORE_EMBEDDED_SESSION_GRAPH_ENABLED}"
            echo "MEMORY_STORE_EMBEDDED_PACK_EXPORT_ENABLED=${MEMORY_STORE_EMBEDDED_PACK_EXPORT_ENABLED}"
            echo "MEMORY_STORE_EMBEDDED_PACK_IMPORT_ENABLED=${MEMORY_STORE_EMBEDDED_PACK_IMPORT_ENABLED}"
          } >> .env

      - name: Start db
        run: docker compose up -d db

      - name: Wait for db
        run: |
          for i in {1..60}; do
            if docker compose exec -T db pg_isready -U aionis -d aionis_memory >/dev/null 2>&1; then
              exit 0
            fi
            sleep 2
          done
          echo "db not ready in time" >&2
          exit 1

      - name: Run migrations
        run: docker compose run --rm migrate

      - name: Contract smoke
        run: npm run -s test:contract

      - name: Startup health + API parity smoke
        run: |
          node dist/index.js >/tmp/backend_parity_api.log 2>&1 &
          API_PID=$!
          echo "${API_PID}" >/tmp/backend_parity_api.pid

          for i in {1..60}; do
            if curl -fsS "http://127.0.0.1:${PORT}/health" >/tmp/backend_parity_health.json 2>/dev/null; then
              break
            fi
            sleep 1
          done

          if [[ ! -s /tmp/backend_parity_health.json ]]; then
            echo "health endpoint never became ready" >&2
            sed -n '1,220p' /tmp/backend_parity_api.log >&2 || true
            exit 1
          fi

          node -e '
            const fs = require("fs");
            const p = JSON.parse(fs.readFileSync("/tmp/backend_parity_health.json", "utf8"));
            const expectedBackend = process.env.MEMORY_STORE_BACKEND;
            if (p.ok !== true) throw new Error("health ok must be true");
            if (p.memory_store_backend !== expectedBackend) {
              throw new Error(`backend mismatch: ${p.memory_store_backend} != ${expectedBackend}`);
            }
            if (typeof p.recall_store_access_capability_version !== "number") throw new Error("missing recall capability version");
            if (typeof p.write_store_access_capability_version !== "number") throw new Error("missing write capability version");
            const recallCaps = p.memory_store_recall_capabilities;
            if (!recallCaps || typeof recallCaps !== "object") throw new Error("missing recall capability object");
            if (typeof recallCaps.debug_embeddings !== "boolean") throw new Error("missing recall capability debug_embeddings");
            if (typeof recallCaps.audit_insert !== "boolean") throw new Error("missing recall capability audit_insert");
            const writeCaps = p.memory_store_write_capabilities;
            if (!writeCaps || typeof writeCaps !== "object") throw new Error("missing write capability object");
            if (typeof writeCaps.shadow_mirror_v2 !== "boolean") throw new Error("missing write capability shadow_mirror_v2");
            const featureCaps = p.memory_store_feature_capabilities;
            if (!featureCaps || typeof featureCaps !== "object") throw new Error("missing feature capability object");
            if (typeof featureCaps.sessions_graph !== "boolean") throw new Error("missing feature capability sessions_graph");
            if (typeof featureCaps.packs_export !== "boolean") throw new Error("missing feature capability packs_export");
            if (typeof featureCaps.packs_import !== "boolean") throw new Error("missing feature capability packs_import");
            const contract = p.memory_store_capability_contract;
            if (!contract || typeof contract !== "object") throw new Error("missing capability contract object");
            const requiredCaps = ["sessions_graph", "packs_export", "packs_import", "debug_embeddings", "shadow_mirror_v2"];
            for (const cap of requiredCaps) {
              const spec = contract[cap];
              if (!spec || typeof spec !== "object") throw new Error(`capability contract missing key: ${cap}`);
              if (typeof spec.failure_mode !== "string") throw new Error(`capability contract missing failure_mode: ${cap}`);
              if (!Array.isArray(spec.degraded_modes)) throw new Error(`capability contract missing degraded_modes: ${cap}`);
            }
            if (contract.shadow_mirror_v2.failure_mode !== "soft_degrade") {
              throw new Error("capability contract mismatch for shadow_mirror_v2.failure_mode");
            }
            for (const cap of ["sessions_graph", "packs_export", "packs_import", "debug_embeddings"]) {
              if (contract[cap].failure_mode !== "hard_fail") {
                throw new Error(`capability contract mismatch for ${cap}.failure_mode`);
              }
            }
            if (p.memory_store_backend === "embedded") {
              if (typeof p.memory_store_embedded_snapshot_max_bytes !== "number") throw new Error("embedded health missing snapshot max bytes");
              if (!p.memory_store_embedded_snapshot_metrics || typeof p.memory_store_embedded_snapshot_metrics !== "object") {
                throw new Error("embedded health missing snapshot metrics object");
              }
            }
          '

          # Deterministic parity case: write one shared node with ready embedding, then recall with same query embedding.
          node -e '
            const fs = require("fs");
            const vec = Array.from({ length: 1536 }, () => 0);
            const writePayload = {
              tenant_id: "parity",
              scope: "backend_parity_smoke",
              actor: "ci",
              input_text: "backend parity smoke write",
              auto_embed: false,
              memory_lane: "shared",
              nodes: [
                {
                  client_id: "parity-node-1",
                  type: "event",
                  title: "Parity Event",
                  text_summary: "Backend parity smoke event",
                  embedding: vec
                }
              ],
              edges: []
            };
            const recallPayload = {
              tenant_id: "parity",
              scope: "backend_parity_smoke",
              query_embedding: vec,
              limit: 5,
              neighborhood_hops: 1
            };
            fs.writeFileSync("/tmp/backend_parity_write.json", JSON.stringify(writePayload));
            fs.writeFileSync("/tmp/backend_parity_recall.json", JSON.stringify(recallPayload));
          '

          curl -fsS -X POST "http://127.0.0.1:${PORT}/v1/memory/write" \
            -H "content-type: application/json" \
            --data-binary "@/tmp/backend_parity_write.json" >/tmp/backend_parity_write_out.json

          curl -fsS -X POST "http://127.0.0.1:${PORT}/v1/memory/recall" \
            -H "content-type: application/json" \
            --data-binary "@/tmp/backend_parity_recall.json" >/tmp/backend_parity_recall_out.json

          node -e '
            const fs = require("fs");
            const writeOut = JSON.parse(fs.readFileSync("/tmp/backend_parity_write_out.json", "utf8"));
            const recallOut = JSON.parse(fs.readFileSync("/tmp/backend_parity_recall_out.json", "utf8"));
            if (typeof writeOut.commit_id !== "string" || writeOut.commit_id.length === 0) {
              throw new Error("write response missing commit_id");
            }
            if (!Array.isArray(writeOut.nodes) || writeOut.nodes.length < 1) {
              throw new Error("write response nodes should be non-empty");
            }
            if (!Array.isArray(recallOut.seeds)) throw new Error("recall seeds missing");
            if (!recallOut.subgraph || !Array.isArray(recallOut.subgraph.nodes)) {
              throw new Error("recall subgraph nodes missing");
            }
            if (recallOut.seeds.length < 1) {
              throw new Error("recall returned no seeds for parity case");
            }
          '

          npm run -s sdk:smoke >/tmp/backend_parity_sdk_smoke.json
          npm run -s sdk:py:smoke >/tmp/backend_parity_py_sdk_smoke.json

          AIONIS_BASE_URL="http://127.0.0.1:${PORT}" \
          CAPABILITY_PROBE_SCOPE="backend_parity_smoke" \
          CAPABILITY_PROBE_TENANT_ID="default" \
          CAPABILITY_PROBE_INCLUDE_SHADOW_SOFT_DEGRADE="false" \
          bash scripts/ci/capability-api-probes.sh >/tmp/backend_parity_capability_api_probe.json

          curl -fsS "http://127.0.0.1:${PORT}/health" >/tmp/backend_parity_health_after.json

          node -e '
            const fs = require("fs");
            const before = JSON.parse(fs.readFileSync("/tmp/backend_parity_health.json", "utf8"));
            const after = JSON.parse(fs.readFileSync("/tmp/backend_parity_health_after.json", "utf8"));
            const tsOut = JSON.parse(fs.readFileSync("/tmp/backend_parity_sdk_smoke.json", "utf8"));
            const pyOut = JSON.parse(fs.readFileSync("/tmp/backend_parity_py_sdk_smoke.json", "utf8"));
            const ensure = (cond, msg) => { if (!cond) throw new Error(msg); };
            const backend = String(after.memory_store_backend || before.memory_store_backend || "");
            const featureCaps = after.memory_store_feature_capabilities || {};
            const recallCaps = after.memory_store_recall_capabilities || {};
            const writeCaps = after.memory_store_write_capabilities || {};

            function assertSdkOut(name, out) {
              ensure(out && out.ok === true, `${name} sdk smoke must be ok=true`);
              const calls = out.calls || {};
              const health = calls.health || {};
              ensure(health.backend === backend, `${name} backend mismatch with /health`);
              const outFeature = health.feature_capabilities || {};
              const outRecall = health.recall_capabilities || {};
              const outWrite = health.write_capabilities || {};

              for (const key of ["sessions_graph", "packs_export", "packs_import"]) {
                ensure(typeof featureCaps[key] === "boolean", `/health feature capability missing ${key}`);
                ensure(outFeature[key] === featureCaps[key], `${name} feature capability mismatch for ${key}`);
              }
              for (const key of ["debug_embeddings", "audit_insert"]) {
                ensure(typeof recallCaps[key] === "boolean", `/health recall capability missing ${key}`);
                ensure(outRecall[key] === recallCaps[key], `${name} recall capability mismatch for ${key}`);
              }
              ensure(
                typeof writeCaps.shadow_mirror_v2 === "boolean",
                "/health write capability missing shadow_mirror_v2",
              );
              ensure(
                outWrite.shadow_mirror_v2 === writeCaps.shadow_mirror_v2,
                `${name} write capability mismatch for shadow_mirror_v2`,
              );

              if (featureCaps.packs_export === true) {
                ensure(
                  typeof calls.pack_export?.manifest_sha256 === "string" && calls.pack_export.manifest_sha256.length > 0,
                  `${name} packs_export enabled path must return manifest sha256`,
                );
              } else {
                ensure(
                  calls.pack_export?.capability_error?.capability === "packs_export",
                  `${name} packs_export disabled path must return capability error`,
                );
              }

              if (featureCaps.sessions_graph === true) {
                ensure(
                  typeof calls.sessions_graph?.events_returned === "number" && calls.sessions_graph.events_returned >= 1,
                  `${name} sessions_graph enabled path must return events`,
                );
              } else {
                ensure(
                  calls.sessions_graph?.capability_error?.capability === "sessions_graph",
                  `${name} sessions_graph disabled path must return capability error`,
                );
              }

              if (featureCaps.packs_import === true) {
                ensure(calls.pack_import?.verified === true, `${name} packs_import enabled verify_only must be verified`);
                ensure(calls.pack_import?.imported === false, `${name} packs_import enabled verify_only must not import`);
              } else {
                ensure(
                  calls.pack_import?.capability_error?.capability === "packs_import",
                  `${name} packs_import disabled path must return capability error`,
                );
              }
            }

            assertSdkOut("ts", tsOut);
            assertSdkOut("py", pyOut);

            if (backend !== "embedded") process.exit(0);
            const b = before.memory_store_embedded_snapshot_metrics || {};
            const a = after.memory_store_embedded_snapshot_metrics || {};
            const maxRounds = Number(after.memory_store_embedded_snapshot_compaction_max_rounds || "8");
            const maxBytes = Number(after.memory_store_embedded_snapshot_max_bytes || "0");
            const maxDroppedNodes = Number(process.env.EMBEDDED_SNAPSHOT_MAX_DROPPED_NODES_GUARD || "32");

            if (typeof a.persist_total !== "number") throw new Error("embedded metrics missing persist_total");
            if (a.persist_total < Number(b.persist_total || 0) + 1) {
              throw new Error(`embedded persist_total did not advance after write: before=${b.persist_total} after=${a.persist_total}`);
            }
            if (typeof a.last_bytes_before_compaction !== "number" || typeof a.last_bytes_after_compaction !== "number") {
              throw new Error("embedded metrics missing compaction byte counters");
            }
            if (a.last_bytes_after_compaction > a.last_bytes_before_compaction) {
              throw new Error("embedded compaction should not increase snapshot size");
            }
            if (a.last_compaction && typeof a.last_compaction.rounds === "number" && a.last_compaction.rounds > maxRounds) {
              throw new Error(`embedded compaction rounds exceeded guard: ${a.last_compaction.rounds} > ${maxRounds}`);
            }
            if (a.last_bytes_before_compaction > maxBytes && (!a.last_compaction || a.last_compaction.applied !== true)) {
              throw new Error("embedded compaction should be marked applied when snapshot exceeds max bytes");
            }
            if (a.last_compaction && Number(a.last_compaction.dropped_nodes || 0) > maxDroppedNodes) {
              throw new Error(
                `embedded compaction dropped too many nodes in one persist: ${a.last_compaction.dropped_nodes} > ${maxDroppedNodes}`,
              );
            }
          '

          kill "${API_PID}" || true
          wait "${API_PID}" || true

      - name: Diagnostics on failure
        if: failure()
        run: |
          docker compose ps || true
          docker compose logs --no-color db || true
          sed -n '1,220p' /tmp/backend_parity_api.log || true
          cat /tmp/backend_parity_capability_api_probe.json || true

      - name: Cleanup
        if: always()
        run: docker compose down -v
