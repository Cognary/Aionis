# Copy to .env and adjust as needed.
# Throughput profile helper:
#   npm run -s env:throughput:prod
#   npm run -s env:throughput:benchmark
#
# For docker-compose.yml defaults:
# user: aionis
# pass: aionis
# db:   aionis_memory
DATABASE_URL=postgres://aionis:aionis@localhost:5432/aionis_memory
# Memory store backend:
# - postgres: reference backend
# - embedded: experimental path (currently postgres-delegated shim; requires explicit enable flag)
MEMORY_STORE_BACKEND=postgres
MEMORY_STORE_EMBEDDED_EXPERIMENTAL_ENABLED=false
DB_POOL_MAX=30
DB_POOL_IDLE_TIMEOUT_MS=30000
DB_POOL_CONNECTION_TIMEOUT_MS=5000

# Runtime mode:
# - local: developer local mode preset
# - service: production service preset (api key auth, strict prod guards)
# - cloud: production cloud preset (api key or jwt auth, strict prod guards)
# Explicit env vars always override mode defaults.
AIONIS_MODE=local

# Runtime environment:
# - dev: local development defaults
# - ci: CI-friendly defaults
# - prod: strict fail-fast safeguards enabled
APP_ENV=dev

# Memory Graph settings
MEMORY_SCOPE=default
MEMORY_TENANT_ID=default
MEMORY_AUTH_MODE=off
# For MEMORY_AUTH_MODE=api_key or api_key_or_jwt:
# {"dev-key":{"tenant_id":"default","agent_id":"agent_a","team_id":"team_default","role":"member"}}
MEMORY_API_KEYS_JSON={}
# For MEMORY_AUTH_MODE=jwt or api_key_or_jwt (HS256 bearer token):
MEMORY_JWT_HS256_SECRET=
MEMORY_JWT_CLOCK_SKEW_SEC=30
EMBEDDING_DIM=1536

# Embeddings
# - fake: deterministic local vectors for dev/testing
# - openai: real embeddings via OpenAI API
# - minimax: real embeddings via MiniMax Embeddings API (model typically `embo-01`)
# - none: disable auto-embedding (client must supply node.embedding)
EMBEDDING_PROVIDER=fake
OPENAI_API_KEY=
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
OPENAI_EMBED_BATCH_SIZE=32

# Embedding HTTP hardening (applies to OpenAI/MiniMax providers)
EMBED_HTTP_TIMEOUT_MS=10000
EMBED_HTTP_MAX_RETRIES=2
EMBED_HTTP_BASE_DELAY_MS=250
EMBED_HTTP_MAX_DELAY_MS=5000
EMBED_HTTP_MAX_CONCURRENCY=8

# MiniMax embeddings (only used when EMBEDDING_PROVIDER=minimax)
MINIMAX_API_KEY=
MINIMAX_GROUP_ID=your_group_id
MINIMAX_EMBED_MODEL=embo-01
MINIMAX_EMBED_TYPE=db
MINIMAX_EMBED_ENDPOINT=https://api.minimax.chat/v1/embeddings

# API
PORT=3001
ADMIN_TOKEN=
# Set true only when running behind a trusted reverse proxy (required for correct client IP attribution on rate limits/audit).
TRUST_PROXY=false
# Comma-separated CORS allowlist. In dev/ci, empty means "*"; in prod, empty means disabled.
# Example: CORS_ALLOW_ORIGINS=https://app.example.com,https://admin.example.com
CORS_ALLOW_ORIGINS=

# Production orchestration hints (script/runtime defaults):
# - set APP_ENV=prod
# - set MEMORY_AUTH_MODE=api_key or jwt (not off)
# - set RATE_LIMIT_BYPASS_LOOPBACK=false
# - run regression/preflight with START_SERVICES_IF_NEEDED=false

# Rate limits (basic protection; per-process in-memory token bucket)
RATE_LIMIT_ENABLED=true
RATE_LIMIT_BYPASS_LOOPBACK=true
RATE_LIMIT_TTL_MS=600000
RECALL_RATE_LIMIT_RPS=10
RECALL_RATE_LIMIT_BURST=20
# Upstream embedding protection for recall_text query embedding.
RECALL_TEXT_EMBED_RATE_LIMIT_RPS=4
RECALL_TEXT_EMBED_RATE_LIMIT_BURST=8
RECALL_TEXT_EMBED_RATE_LIMIT_MAX_WAIT_MS=600
DEBUG_EMBED_RATE_LIMIT_RPS=0.2
DEBUG_EMBED_RATE_LIMIT_BURST=2
WRITE_RATE_LIMIT_RPS=5
WRITE_RATE_LIMIT_BURST=10
# Optional write smoothing (ms): if throttled, wait briefly and retry once before returning 429.
WRITE_RATE_LIMIT_MAX_WAIT_MS=200

# Tenant-level quotas (independent from per-IP rate limit)
TENANT_QUOTA_ENABLED=true
TENANT_RECALL_RATE_LIMIT_RPS=30
TENANT_RECALL_RATE_LIMIT_BURST=60
TENANT_RECALL_TEXT_EMBED_RATE_LIMIT_RPS=8
TENANT_RECALL_TEXT_EMBED_RATE_LIMIT_BURST=16
TENANT_RECALL_TEXT_EMBED_RATE_LIMIT_MAX_WAIT_MS=800
TENANT_DEBUG_EMBED_RATE_LIMIT_RPS=1
TENANT_DEBUG_EMBED_RATE_LIMIT_BURST=4
TENANT_WRITE_RATE_LIMIT_RPS=10
TENANT_WRITE_RATE_LIMIT_BURST=20
TENANT_WRITE_RATE_LIMIT_MAX_WAIT_MS=300
# control-plane tenant quota profile cache TTL (ms)
CONTROL_TENANT_QUOTA_CACHE_TTL_MS=30000
# request telemetry retention window used by hosted dashboard APIs (hours)
CONTROL_TELEMETRY_RETENTION_HOURS=720
# max rows deleted per retention cleanup pass
CONTROL_TELEMETRY_PURGE_BATCH_LIMIT=20000

# recall_text query embedding cache (protect upstream embedding RPM under repeated queries)
RECALL_TEXT_EMBED_CACHE_ENABLED=true
RECALL_TEXT_EMBED_CACHE_MAX_KEYS=2000
RECALL_TEXT_EMBED_CACHE_TTL_MS=600000
# Server-side batching for recall_text query embeddings (reduces upstream RPM/429).
RECALL_TEXT_EMBED_BATCH_ENABLED=true
RECALL_TEXT_EMBED_BATCH_MAX_SIZE=24
RECALL_TEXT_EMBED_BATCH_MAX_WAIT_MS=8
RECALL_TEXT_EMBED_BATCH_MAX_INFLIGHT=4
RECALL_TEXT_EMBED_BATCH_QUEUE_MAX=12000
RECALL_TEXT_EMBED_BATCH_QUEUE_TIMEOUT_MS=5000
# API inflight gates (coarse backpressure under high concurrency).
API_RECALL_MAX_INFLIGHT=256
API_RECALL_QUEUE_MAX=6000
API_RECALL_QUEUE_TIMEOUT_MS=2000
API_WRITE_MAX_INFLIGHT=96
API_WRITE_QUEUE_MAX=3000
API_WRITE_QUEUE_TIMEOUT_MS=2000
# Server-side default recall profile (used when request omits recall knobs):
# - lite: local-first, lower-cost bounded recall
# - strict_edges: production default (quality-focused, robust seed quality)
# - quality_first: broader recall coverage profile
# - legacy: historical defaults (pre-profile)
MEMORY_RECALL_PROFILE=strict_edges
# Layered profile policy JSON (optional): global default -> endpoint -> tenant -> tenant+endpoint.
# keys:
# - endpoint: {"recall":"strict_edges","recall_text":"quality_first"}
# - tenant_default: {"vip-tenant":"quality_first"}
# - tenant_endpoint: {"vip-tenant":{"recall":"strict_edges","recall_text":"quality_first"}}
MEMORY_RECALL_PROFILE_POLICY_JSON={}
# Queue-pressure adaptive downgrade:
# - enabled: when recall inflight queue wait is high, fallback to target profile for non-pinned requests
# - wait_ms: trigger threshold
MEMORY_RECALL_ADAPTIVE_DOWNGRADE_ENABLED=true
MEMORY_RECALL_ADAPTIVE_WAIT_MS=200
MEMORY_RECALL_ADAPTIVE_TARGET_PROFILE=strict_edges
# Queue-pressure hard-cap (second-stage protection for long-tail latency):
# - applies only when recall knobs are not explicitly pinned
# - trims expansion budgets and raises edge-quality floors under sustained queue wait
MEMORY_RECALL_ADAPTIVE_HARD_CAP_ENABLED=true
MEMORY_RECALL_ADAPTIVE_HARD_CAP_WAIT_MS=600
MEMORY_RECALL_ADAPTIVE_HARD_CAP_LIMIT=16
MEMORY_RECALL_ADAPTIVE_HARD_CAP_NEIGHBORHOOD_HOPS=1
MEMORY_RECALL_ADAPTIVE_HARD_CAP_MAX_NODES=40
MEMORY_RECALL_ADAPTIVE_HARD_CAP_MAX_EDGES=50
MEMORY_RECALL_ADAPTIVE_HARD_CAP_RANKED_LIMIT=90
MEMORY_RECALL_ADAPTIVE_HARD_CAP_MIN_EDGE_WEIGHT=0.25
MEMORY_RECALL_ADAPTIVE_HARD_CAP_MIN_EDGE_CONFIDENCE=0.25
# Stage-1 safety net: if ANN returns zero seeds, run one exact KNN fallback query.
MEMORY_RECALL_STAGE1_EXACT_FALLBACK_ON_EMPTY=true
# Optional default context token budget for recall_text (0 disables request-default compaction).
MEMORY_RECALL_TEXT_CONTEXT_TOKEN_BUDGET_DEFAULT=0

# Safety / policy
PII_REDACTION=true
ALLOW_CROSS_SCOPE_EDGES=false
MAX_TEXT_LEN=8000

# Abstraction policy profile (topic clustering + compression defaults):
# - conservative: higher precision, lower abstraction churn
# - balanced: default production profile
# - aggressive: larger coverage, stronger compression rollups
MEMORY_ABSTRACTION_POLICY_PROFILE=balanced

# Topic clustering (online kNN)
TOPIC_SIM_THRESHOLD=0.78
TOPIC_MIN_EVENTS_PER_TOPIC=5
TOPIC_CLUSTER_BATCH_SIZE=200
TOPIC_MAX_CANDIDATES_PER_EVENT=5
# Strategy:
# - online_knn: current online nearest-topic assignment
# - offline_hdbscan: reserved mode; currently falls back to online_knn with explicit runtime note
TOPIC_CLUSTER_STRATEGY=online_knn

# Default behavior on write:
# if true and the write includes event nodes, trigger topic clustering automatically.
AUTO_TOPIC_CLUSTER_ON_WRITE=true
# if true, enqueue to outbox (recommended). if false, run synchronously in the write request.
TOPIC_CLUSTER_ASYNC_ON_WRITE=true

# Outbox worker
OUTBOX_POLL_INTERVAL_MS=1000
OUTBOX_BATCH_SIZE=20
OUTBOX_CLAIM_TIMEOUT_MS=300000
OUTBOX_MAX_ATTEMPTS=25

# Phase C (shadow cutover prep): mirror /write artifacts to *_v2 tables.
MEMORY_SHADOW_DUAL_WRITE_ENABLED=false
# If true, any dual-write mirror failure fails the request; if false, mirror failures are logged and ignored.
# Requires MEMORY_SHADOW_DUAL_WRITE_ENABLED=true.
MEMORY_SHADOW_DUAL_WRITE_STRICT=false

# Long-term memory tiering (Phase 1)
MEMORY_SALIENCE_DECAY_FACTOR=0.995
MEMORY_TIER_WARM_BELOW=0.35
MEMORY_TIER_COLD_BELOW=0.12
MEMORY_TIER_ARCHIVE_BELOW=0.03
MEMORY_TIER_WARM_INACTIVE_DAYS=14
MEMORY_TIER_COLD_INACTIVE_DAYS=45
MEMORY_TIER_ARCHIVE_INACTIVE_DAYS=120
MEMORY_TIER_MAX_DAILY_MUTATION_RATIO=0.05
# Scope-level memory budgets (0 = disabled)
MEMORY_SCOPE_HOT_NODE_BUDGET=0
MEMORY_SCOPE_ACTIVE_NODE_BUDGET=0
# Adaptive decay (access + feedback)
MEMORY_ADAPTIVE_DECAY_ENABLED=true
MEMORY_ADAPTIVE_RECENT_DAYS=7
MEMORY_ADAPTIVE_RECENT_SCALE=0.6
MEMORY_ADAPTIVE_FEEDBACK_POS_STRENGTH=0.5
MEMORY_ADAPTIVE_FEEDBACK_NEG_STRENGTH=1.0
MEMORY_ADAPTIVE_DECAY_SCALE_MIN=0.25
MEMORY_ADAPTIVE_DECAY_SCALE_MAX=2.0

# Compression rollup (Phase 2)
MEMORY_COMPRESSION_LOOKBACK_DAYS=30
MEMORY_COMPRESSION_TOPIC_MIN_EVENTS=4
MEMORY_COMPRESSION_MAX_TOPICS_PER_RUN=50
MEMORY_COMPRESSION_MAX_EVENTS_PER_TOPIC=12
MEMORY_COMPRESSION_MAX_TEXT_LEN=1800

# Consolidation candidate scoring (Phase 3 shadow mode)
MEMORY_CONSOLIDATION_MIN_VECTOR_SIM=0.86
MEMORY_CONSOLIDATION_MIN_SCORE=0.82
MEMORY_CONSOLIDATION_MAX_ANCHORS=300
MEMORY_CONSOLIDATION_NEIGHBORS_PER_NODE=8
MEMORY_CONSOLIDATION_MAX_PAIRS=200
MEMORY_CONSOLIDATION_REDIRECT_MAX_ALIASES=200
MEMORY_CONSOLIDATION_REDIRECT_MAX_EDGES_PER_ALIAS=2000
MEMORY_CONSOLIDATION_BLOCK_CONTRADICTORY=true
MEMORY_CONSOLIDATION_CONFLICT_MIN_SHARED_TOKENS=1
MEMORY_CONSOLIDATION_CONFLICT_NEGATION_LEXICAL_MIN=0.5

# Release credentials (optional; for local publish scripts only)
# NPM_TOKEN=npm_xxx
# TWINE_USERNAME=__token__
# TWINE_PASSWORD=pypi-xxx
# GHCR_USERNAME=<github_user>
# GHCR_TOKEN=<ghcr_pat_or_github_token>
# IMAGE_REPO=ghcr.io/<owner>/aionis-memory-graph
